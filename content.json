{"meta":{"title":"Lamb Python Blog","subtitle":"登高必自卑 行远必自迩","description":"","author":"CodeLamb","url":"http://PythonLamb.github.io","root":"/"},"pages":[{"title":"标签","date":"2021-10-03T13:31:42.000Z","updated":"2021-10-04T08:45:01.464Z","comments":true,"path":"tags/index.html","permalink":"http://pythonlamb.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-10-03T13:30:15.000Z","updated":"2021-10-03T13:35:38.446Z","comments":true,"path":"categories/index.html","permalink":"http://pythonlamb.github.io/categories/index.html","excerpt":"","text":""},{"title":"测试页面","date":"2021-04-21T09:14:49.000Z","updated":"2021-04-21T09:14:49.350Z","comments":true,"path":"测试页面/index.html","permalink":"http://pythonlamb.github.io/%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A2/index.html","excerpt":"","text":""}],"posts":[{"title":"redis数据库【第四期】","slug":"redis数据库【第四期】","date":"2021-10-06T12:37:51.000Z","updated":"2021-10-06T12:52:30.762Z","comments":true,"path":"2021/10/06/redis数据库【第四期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/redis%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%90%E7%AC%AC%E5%9B%9B%E6%9C%9F%E3%80%91/","excerpt":"本文主讲：数据持久化存储、redis数据库主从概念、设置redis主从集群等….","text":"本文主讲：数据持久化存储、redis数据库主从概念、设置redis主从集群等…. redis数据持久化存储 为什么要对 redis 数据进行持久化存储 答：因为 redis 数据库是通过内存进行数据的增删改查的，如果遇到突然断电的情况，存放在redis数据库里面的数据将会消失，这时就不得不考虑数据持久化了 redis 进行数据持久化存储的两种方式 第一种方式：快照存储 存储指令：bgsave 关于快照存储的一些设置： 注意： 1：dump.rdb文件是通过保存数据的 key 和value 进行数据持久化存储的 2：快照存储方式还是会造成一部分数据的丢失，因为如果在部分时间内没有超过快照持久化存储的条件，那么是不会快照存储数据的 第二种方式：aof 持久化存储【实时存储】 打开 aof 持久化存储 注意： 1：aof 持久化存储数据是通过把 redis 命令存放到 appendonly.aof 文件内 2：在 redis4.0 版本之前如果开启了 aof 持久化存储，那么快照持久化存储就会失效 redis 进行数据持久化存储的两种方式优缺点 快照存储： 优点：长期进行数据的持久化存储不会占用太多内存缺点：在特殊情况下还是会造成数据的缺失【更改key的数量未超过时间阈值时断电】 aof存储： 优点：可以实时进行数据的存储缺点：长期进行数据的持久化存储占用太多内存 redis数据库主从概念 单台 redis 主机数据库的优缺点 优点：部署容易，数据的操作【增、删、改】也容易 缺点：数据量特别大的时候，读取和写入数据压力特别大 为什么要为 redis 数据库设置主从【集群】 答：因为 redis 经常用作缓存【大量的读操作】，这样如果读取数据的压力很大的时候，单台 redis 服务器就不够看了，因此就要布置 redis 数据库的主从集群 redis 数据库主从的实现 实现：准备多台用于部署 redis 集群的服务器，一般是单数，设置其中一台为主服务器，另外的其他服务器设置为从服务器 注意：为什么设置 redis 集群，服务器的数量时是单数，因为当集群中有一半的服务器不能用的时候，整个集群系统就不能用了，设置双数那么不是浪费服务器资源吗！ redis数据库集群优点： 1：提高可用性，一台服务器宕机，其他服务器也可以完成这个操作 2：分散 redis 数据库大量操作的压力！ 图示： 注意：从服务器也可以进行写操作，但是 Redis 做缓存的时候，从服务器只执行读操作是最理想的 设置redis主从集群 为多台 redis 服务器设置主从集群的步骤【主服务器与从服务器的设置】 第一步：准备多台装了一个版本的 redis 的服务器 第二步：主服务器的配置【修改主服务器的 redis.conf 文件】 配置可以远程连接 设置主服务器的密码 开启redis-server后台启动 配置完之后重启主服务器 第三步：从服务器的配置【修改从服务器的 redis.conf 文件】 配置可以远程连接 开启 redis-server 后台启动 设置从服务器连接哪台主服务器以及连接那台主服务器的密码 设置从服务器的读写权限 配置完之后重启所有的从服务器 第四步：重新连接主服务器以及所有的从服务器 设置完 redis 主从集群之后，怎么查看当前服务器是主服务器还是从服务器 第一步：连接待查看的 redis 数据库 连接指令：redis-cli 第二步：输入下面指令即可查看 查看指令：info replications 持续更新中……","categories":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}],"tags":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}]},{"title":"redis数据库【第三期】","slug":"redis数据库【第三期】","date":"2021-10-06T12:27:03.000Z","updated":"2021-10-06T12:36:00.618Z","comments":true,"path":"2021/10/06/redis数据库【第三期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/redis%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%90%E7%AC%AC%E4%B8%89%E6%9C%9F%E3%80%91/","excerpt":"本文主讲内容：数据的时效时间及其redis与python进行交互！","text":"本文主讲内容：数据的时效时间及其redis与python进行交互！ 数据的失效时间 什么是数据的失效时间 答：为数据设置一个时限，超过这个时限，这个数据就会自动删除，应用场景：验证码！ 在 redis 数据库为 key【数据】 设置失效时间的两种方式 第一种形式：创建数据时为指定 key 设置失效时间 语法： 1：set key value ex 30 为这个 key 设置失效时间，失效时间为 30 秒 2：set key value px 300000 为这个 key 设置失效时间，失效时间为 300000 毫秒 第二种形式：为已经创建的 key 添加失效时间 语法：expire 已经存在的key 30 【为这个 key 设置失效时间，失效时间为 30 秒】 如何在 redis 数据库内查询某个 key 的失效时间还剩多长 语法： 1：ttl 指定key 【查询指定key还剩多少时间失效，查询单位显示为秒】2：pttl 指定key 【查询指定key还剩多少时间失效，查询单位显示为毫秒】 结果： 正数：查询某个键还有多少时间失效，如果显示为正数就说明还有正数这些时间失效 -1：查询某个键还有多少时间失效，如果显示为 -1 就说明这个键永远不失效 -2：查询某个键还有多少时间失效，如果显示为 -2 就说明这个键已经永远不失效 图示： 在 redis 数据库内创建数据时，nx 参数与 xx 参数的作用 语法： 1：set key value nx 【如果这个key在redis数据库内不存在那么创建这个 key 成功】 2：set key value xx 【如果这个key在redis数据库内存在那么创建这个 key 成功】 3：set key value ex 30 nx 【如果这个key在redis数据库内不存在那么创建这个 key 成功，并且在30秒后这个key失效】 4：set key value ex 30 xx 【如果这个key在redis数据库内存在那么创建这个 key 成功，并且在30秒后这个key失效】 图示： 应用场景：前两种语法用于加锁的机制上面，后两种情况用于锁机制避免出现死锁的情况 redis数据库与python交互【重点】 redis 与 python 进行交互的步骤 第一步：安装 python 的 redis 第三方库 → pip3 install redis 第二步：导入 redis 第三方数据库 → import redis 第三步：创建 redis 连接数据库对象 → red_obj = redis.Redis(host=主机，port=端口号，db=数据库号) 注意：redis默认有16个数据库，也就是数据库号可以是 0-15，一般选择0 redis的端口号是6379（整形） 第四步：利用创建的 redis 数据库对象对数据库进行操作 → result = red_obj.sadd(“name：age” , “21”) 向外键名为name的集合内插入键为age值为“21”的一条数据 注意：redis内的集合数据类型具有数据唯一性以及无序性的特征，如果上述插入的数据在集合内存在返回0，不存在返回数据1 截图： redis 与 python 交互后通过代码对redis数据库的操作 ps：确保对 redis 数据库操作之前已经成功通过代码连接上 redis 数据库 操作截图【对redis数据库内的string数据类型进行操作】： ps：下面代码中的 sr 是建立数据库连接的对象 持续更新中……","categories":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}],"tags":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}]},{"title":"redis数据库【第二期】","slug":"redis数据库【第二期】","date":"2021-10-06T12:06:54.000Z","updated":"2021-10-06T12:23:00.049Z","comments":true,"path":"2021/10/06/redis数据库【第二期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/redis%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%90%E7%AC%AC%E4%BA%8C%E6%9C%9F%E3%80%91/","excerpt":"本文主讲：连接redis数据库及其redis数据库的数据类型","text":"本文主讲：连接redis数据库及其redis数据库的数据类型 通过指令连接 redis 数据库 第一步：开启 redis 服务器 指令：redis-server注意：redis-server中的 - 两边没有空格 第二步：开启 redis 客户端 指令：redis-cli -a 设置的密码注意：如果设置了密码需要在 redis-cli 后面跟上 -a 设置的密码 第三步：输入 ping 即可进入 redis 数据库 redis-cli 的指令选项都是干什么的 选项： -a：用于认证密码 例如 redis-cli -a yhq256112 -h：用于指定连接的 ip 地址 例如 redis-cli -h 192.168.1.8 【可用于远程连接数据库】 -p：用于指定redis数据库监听的端口号 例如 redis-cli -p 6379 【注意端口号一般都指定为6379】 远程连接数据库 redis数据库的数据类型 如何向 redis 指定数据库添加数据 答：在连接上数据库之后执行下面指令即可选择指定数据库存放数据【redis默认有16个逻辑数据库，第一个数据库索引编号为0】 指令：select 2 指令解析：选择索引编号为2的数据库【第三个数据库】存放数据 redis 数据库中 String 数据类型的操作 数据结构：{“key”：“value”} 操作语法： 添加单条数据：set key value 取出单条数据：get key 添加多条数据：mset key1 value1 key2 value2 取出多条数据：mget key1 key2 图示： redis 数据库中 Hashes【散列】 数据类型的操作 数据结构：{“外key”：{ “key1”：“value1”，“key2”：“value2”}} 也就是字典的嵌套 操作语法： 添加单条数据：hset 外key key1 value1 取出单条数据：hget 外key key1 添加多条数据：hmset 外key key1 value1 key2 values 取出多条数据：hmget 外key key1 key2 取出外key对应的所有数据：hgetall 外key 删除数据：hdel 外key key1 key2 图示： redis 数据库中 List【列表】 数据类型的操作 数据结构：{“外键key”：“ [ ‘value1’,’value2’,’value3’ ] ”} PS：value的值可以重复，并且列表是有序的 操作语法： 在列表的左侧插入数据【单条和多条】：lpush 外键 value1 value2….. 在列表的右侧插入数据【单条和多条】：rpush 外键 value1 value2….. 查找列表内指定索引的值：lrange 外键 开始索引 结束索引 计算外键对应列表的长度：llen 外键 删除指定数量的数据：lrem 外键 删除数量 待删除的value ​ 图示： 注意事项： 1：在列表内插入【两端插入】数据，新来的数据总是占两端端点处！ 2：删除数据时，如果删除数据的数量大于现存数据量，那么这些数据全部删除，如果小于现存数据量，就从列表前面【左端】依次开始删除 redis 数据库中 Sets【集合】 数据类型的操作 数据结构：{“外键key”：“ （ ‘value1’,’value2’,’value3’ ） ”} PS：value的值不可重复，并且set是没有顺序的 操作语法： 向外键key内的集合添加数据【单条和多条】：sadd 外键 value1 value2 value3 查找外键内集合的所有数据：smembers 外键 计算外键内集合的长度：scard 外键 删除外键内集合中指定数据：srem 外键 待删除的数据1 待删除的数据2 图示： 注意： 1：因为集合是无序的，索引不支持索引，因此不能用索引查询 2：集合说是无序的【应用层】，其实底层系统也帮集合数据排好序了 redis 数据库中 Sorted Set【有序集合】 数据类型的操作 数据结构：{“外键key”：“ （ ‘value1’,’value2’,’value3’ ） ”} PS：value的值不可重复，并且set是有顺序的【根据权重从小到大排序】 操作语法： 向外键key所对应的有序集合内插入有序数据【单条和多条】：zadd 外键key 值1的权重 值1 值2的权重 值2 查找外键所对应的有序集合内指定索引的数据：zrange 外键 开始索引 结束索引 计算外键内有序集合的长度：zcard 外键 删除外键内有序集合中指定数据：zrem 外键 待删除的数据1 待删除的数据2 图示： redis 数据库中查找与删除的通用操作 通用操作：指令作用域是五个数据类型，而不仅仅是指定一个 查找所有键【字符串类型】与外键【哈希、列表、集合、有序集合】指令：keys * 删除指定键【字符串类型】与外键【哈希、列表、集合、有序集合】指令：del 键1 键2 外键1…. 图示： reids 数据库中层级目录的操作 什么是层级目录：在 redis 数据库内层级目录就类似于文件夹的分级结构！ 为什么要设置层级目录：方便同类型数据的查询 图示： 层级目录文件结构： 持续更新中…….","categories":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}],"tags":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}]},{"title":"redis数据库【第一期】","slug":"redis数据库【第一期】","date":"2021-10-06T11:40:42.000Z","updated":"2021-10-06T11:59:57.682Z","comments":true,"path":"2021/10/06/redis数据库【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/redis%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"*本文教程主讲：1：什么是redis2：怎么安装redis3：如何配置redis*","text":"*本文教程主讲：1：什么是redis2：怎么安装redis3：如何配置redis* 什么是 redis 数据库 注意： 1：redis 是非关系型数据库（以键值对的形式存储数据） 2：互联网三高：高并发、高性能、高可用 redis的安装 windows安装方式： 安装链接：https://blog.csdn.net/weixin_41996197/article/details/89427073?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162640227416780255259420%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=162640227416780255259420&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-89427073.pc_search_result_control_group&amp;utm_term=WIN10%E6%80%8E%E4%B9%88%E5%AE%89%E8%A3%85redis&amp;spm=1018.2226.3001.4187 linux安装指令： 1sudo apt-get install redis-server 配置redis 注意：修改完 redis.conf 文件参数后一定要重启 redis 数据库才能生效 如何配置reids完成redis的后台启动 第一步：在linux系统主目录下找到并打开 /etc/redis/redis.conf 文件 【windows系统是redis.windows-service.conf 文件】 第二步：对文件内的 daemonize 的参数进行修改即可【windows系统不支持这个参数】 第三步：通过指令重启 redis 数据库即可生效 重启指令： 1sudo &#x2F;etc&#x2F;init.d&#x2F;redis-server restart 如何配置reids实现数据库可被远程连接 第一步：在linux系统主目录下找到并打开 /etc/redis/redis.conf 文件 第二步：对下面两个参数进行修改即可 第三步：通过指令重启 redis 数据库即可生效 重启指令： 1sudo &#x2F;etc&#x2F;init.d&#x2F;redis-server restart 如何配置redis实现为数据库增加密码保护 第一步：在linux系统主目录下找到并打开 /etc/redis/redis.conf 文件 第二步：修改 requirepass 参数即可 第三步：通过指令重启 redis 数据库即可生效 重启指令： 1sudo &#x2F;etc&#x2F;init.d&#x2F;redis-server restart 如何配置redis实现改变数据库的数量 注意：redis内数据库只是逻辑数据库，并不是真实的数据库 第一步：在linux系统主目录下找到并打开 /etc/redis/redis.conf 文件 第二步：修改 database 参数即可 第三步：通过指令重启 redis 数据库即可生效 重启指令： 1sudo &#x2F;etc&#x2F;init.d&#x2F;redis-server restart 配置生效：","categories":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}],"tags":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"}]},{"title":"爬虫工程师查缺补漏大纲流程图","slug":"爬虫工程师查缺补漏大纲流程图","date":"2021-10-06T11:23:02.000Z","updated":"2021-10-06T11:26:46.679Z","comments":true,"path":"2021/10/06/爬虫工程师查缺补漏大纲流程图/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%9F%A5%E7%BC%BA%E8%A1%A5%E6%BC%8F%E5%A4%A7%E7%BA%B2%E6%B5%81%E7%A8%8B%E5%9B%BE/","excerpt":"目前距离爬虫工程师的道路还有很远，下面是我自己整理待学习的爬虫资源目录，砥砺前行！","text":"目前距离爬虫工程师的道路还有很远，下面是我自己整理待学习的爬虫资源目录，砥砺前行！ 爬虫路漫漫……..","categories":[{"name":"爬虫练习","slug":"爬虫练习","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0/"}],"tags":[{"name":"爬虫练习","slug":"爬虫练习","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0/"}]},{"title":"爬虫常用解析类库——pyquery","slug":"爬虫常用解析类库——pyquery","date":"2021-10-06T10:51:54.000Z","updated":"2021-10-06T11:08:47.809Z","comments":true,"path":"2021/10/06/爬虫常用解析类库——pyquery/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E8%A7%A3%E6%9E%90%E7%B1%BB%E5%BA%93%E2%80%94%E2%80%94pyquery/","excerpt":"本文主讲内容：爬虫解析数据的类库pyquery的简单使用","text":"本文主讲内容：爬虫解析数据的类库pyquery的简单使用 怎么安装 pyquery 安装指令： 1pip3 install pyquery 使用 pyquery 匹配数据的步骤 第一步：导入模块 语法： 1from pyquery import PyQuery as pq 第二步：通过 PyQuery 类实例化一个 PyQuery 对象 语法： 123doc = pq（网页源代码）doc = pq（url = “网站”，encoding = “字符集”）doc = pq（filename= html文件，encoding = “字符集”） 注意：第一种实例化 PyQuery 对象的方法最常用，因为像网站发送请求时可以更加灵活的控制请求头【headers】，第三种方法也挺常用的 第三步：通过实例化对象的属性和方法匹配标签以及数据！ 其中一种匹配数据的语法示例：doc(“li”) 表示匹配html源代码内的所有 li 标签并且返回一个 pyquery类型的对象 pyquery 匹配数据的几种方法 1.匹配 PyQuery 对象中的指定标签 语法： 12result = doc（“li”）result = doc（“li[class = &#x27;item-5&#x27;]”） 语法解释： 1：匹配数据类型为pyquery的指定标签【doc对象】的所有li标签，其中 result 的类型是 pyquery 类型 2：匹配数据类型为pyquery的指定标签【doc对象】的class属性为“item-5”的所有li标签，其中 result 的类型是 pyquery 类型 语法1 语法2： 2.通过css选择器语法匹配指定标签 语法：result = doc（“.item-1”） 或者 result = doc（“#item-1”） 其中 . 代表class，#代表id 【参考bs4】 解释：匹配doc对象中【html源码】的所有class属性为“item-1”的标签，其中 result 的类型是 pyquery 类型 3.查找某标签下的子标签 语法：数据类型为pyquery的指定标签.find（“.item-1”） 解释：在数据类型为pyquery的指定标签下通过 find 方法找到class属性为“item-1”的所有标签，并且返回为一个 pyquery类型的数据 4.查找某标签下的父【祖父】标签 语法： 12数据类型为pyquery的指定标签.parent（）数据类型为pyquery的指定标签.parents（） 解释： 1：通过数据类型为pyquery的指定标签的partent方法找到这个标签的父标签，并返回一个数据类型为 pyquery 的数据 2：通过数据类型为pyquery的指定标签的partents方法找到这个标签的父与祖父标签，并返回一个数据类型为 pyquery 的数据 5.可遍历【迭代】的标签对象 语法：数据类型为pyquery的指定标签（“.item-0”）.items（） 解释：匹配数据类型为pyquery的指定标签里面的所有class属性为“item-0”的标签，并且将这些标签存放到一个可迭代对象里面而不是返回一个数据类型为 pyquery的数据 注意：循环遍历出去的class属性为“item-0”的标签还是一个数据类型为pyquery的数据 6.获取数据类型为pyquery的指定标签内的属性 语法： 12数据类型为pyquery的指定标签.attr（&#x27;href&#x27;）数据类型为pyquery的指定标签.attr.href 解释：匹配数据类型为 pyquery 的指定标签内的 href 属性的值 7.获取指定标签内的文本数据 语法：数据类型为pyquery的指定标签.text（） 语法解释：获取 数据类型为pyquery的指定标签 的文本数据值 8.获取指定标签下的html标签 语法：数据类型为pyquery的指定标签.html（） 语法解释：获取数据类型为pyquery的指定标签下一级的html标签 9.DOM操作【向数据类型为 pyquery 的指定标签内增加类和移除类】 语法： 12数据类型为 pyquery 的指定标签.add_class（&#x27;love&#x27;）数据类型为 pyquery 的指定标签.remove_class（&#x27;love&#x27;） 语法解释： 1：为数据类型为 pyquery 的指定标签增加一个值为“love”的class属性2：为数据类型为 pyquery 的指定标签移除一个值为“love”的class属性 注意：移除和增加指定标签的属性都是浅的，不是永久的作用范围只是当前程序，程序结束回归原样【每增加没移除】 语法1截图： 语法2截图： 持续更新中……","categories":[{"name":"常用类库篇","slug":"常用类库篇","permalink":"http://pythonlamb.github.io/categories/%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%BA%93%E7%AF%87/"}],"tags":[{"name":"常用类库篇","slug":"常用类库篇","permalink":"http://pythonlamb.github.io/tags/%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%BA%93%E7%AF%87/"}]},{"title":"爬虫常用解析类库——bs4","slug":"爬虫常用解析类库——bs4","date":"2021-10-06T10:39:45.000Z","updated":"2021-10-06T10:50:49.701Z","comments":true,"path":"2021/10/06/爬虫常用解析类库——bs4/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E8%A7%A3%E6%9E%90%E7%B1%BB%E5%BA%93%E2%80%94%E2%80%94bs4/","excerpt":"本文主讲：bs4解析数据库的简答使用！","text":"本文主讲：bs4解析数据库的简答使用！ 使用 bs4 库需要什么环境 环境： bs4库： 1pip3 install bs4 lxml解释器： 1pip3 install lxml bs4 库匹配数据的具体步骤 第一步：从 bs4 库内导入 BeautifulSoup 类 1from bs4 import BeautifulSoup 第二步：获取网页源代码文本数据 1html_data = requests.get(url = url,headers = headers).text 第三步：创建美味汤对象【soup】 参数： html_data：网页源代码文本数据lxml：解释器，它是固定不变的 1soup_obj = BeautifulSoup(html_data,&quot;lxml&quot;) 第四步：利用美味汤对象的属性以及方法匹配自己想要的数据 下面代码是利用美味汤对象的css选择器方法匹配 html 标签内容 1dd_tag_list = soup_obj.select(&quot;.board-wrapper&gt;dd&quot;) 总体截图 bs4 库匹配标签的方法 标签匹配：美味汤对象 . html标签名示例：soup . div解释：匹配 html 源代码中第一个 div 标签注意：这样匹配的是整个标签，即数据加上标签符号, 例如 啦啦啦 标签属性匹配：美味汤对象.find（html标签名，标签属性名 = 标签属性值）示例：soup.find（“div”，class_ = “str”）代码解释：查找 class 属性为 str 的 div 标签 注意1：为什么 class 后面要加上下划线 _ , 因为 class 是关键字2：这样匹配的数据也是整个标签 美味汤对象.find_all（html标签名，标签属性名 = 标签属性值）示例：soup.find_all（“div”，class_ = “str”）代码解释：查找所有 class 属性为 str 的 div 标签，并且将所有匹配的标签放在一个列表内 css选择器匹配【标签、类、id、层级匹配】： 标签选择器匹配：美味汤对象.select（“div”）代码解释：匹配所有的div标签 类选择器匹配：美味汤对象.select（“.title”）代码解释：匹配 class 属性为 title 的标签，并将标签存放至列表内！.符号 代表类选择器注意：选择器匹配数据标签后都将标签存放至列表内 id选择器匹配：美味汤对象.select（“#title”）代码解释：匹配 id 属性为 title 的标签，并将标签存放至列表内！#符号 代表 id 选择器 大于号层级选择器：美味汤对象.select（“.title &gt; li &gt; lv”）代码解释：匹配 class 属性为 title 的标签里面所有的的 li 标签内的所有 lv 标签，并将标签存放至列表内！&gt; 符号表示lv标签在li标签内！也就是子标签 空格号层级选择器：美味汤对象.select（“.title lv”）代码解释：匹配 class 属性为 title 的标签里面的所有 lv 标签【多级匹配】，并将标签存放至列表内！空格符号表示lv标签不是class属性为title的标签的直系标签，可能是孙标签或者重孙标签 bs4 库匹配标签内数据的方法 持续更新中…….","categories":[{"name":"常用类库篇","slug":"常用类库篇","permalink":"http://pythonlamb.github.io/categories/%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%BA%93%E7%AF%87/"}],"tags":[{"name":"常用类库篇","slug":"常用类库篇","permalink":"http://pythonlamb.github.io/tags/%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%BA%93%E7%AF%87/"}]},{"title":"爬虫主流框架分享--Pyspider","slug":"爬虫主流框架分享-Pyspider","date":"2021-10-06T10:28:45.000Z","updated":"2021-10-06T10:37:10.252Z","comments":true,"path":"2021/10/06/爬虫主流框架分享-Pyspider/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB-Pyspider/","excerpt":"本文主讲内容：pyspider框架的简单使用","text":"本文主讲内容：pyspider框架的简单使用 注意：pyspider 框架主要在windows平台上使用！ 怎么 windows 安装 pyspider 框架 安装指令： 1pip3 install pyspider 注意： 1：如果以上指令安装出现错误请手动下载后缀为 whl 的 pyspider 文件手动安装即可，手动安装过成功出现错误上网查一查即可 2：安装好 pyspider 框架后，一定要先关闭所有浏览器之后再从终端输入指令 ：pyspider all 运行这个框架 3：安装好 pyspider 框架后终端输入 pyspider all 指令出现 SyntaxError: invalid syntax 错误请看下面这篇文章进行解决 文章链接：https://blog.csdn.net/chenjiale_123/article/details/107730039 4：通过指令 pyspider all 运行 pyspider 框架时如果出现 ImportError: cannot import name ‘ContextVar’ from ‘werkzeug.local’ 这样的错误请看下面的文章 文章链接：https://blog.csdn.net/qq_46485161/article/details/118860024?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=pyspider%20all%E5%91%BD%E4%BB%A4%E6%8A%A5%E9%94%99%E5%A6%82%E4%B8%8B%EF%BC%9AImportError&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-118860024.first_rank_v2_pc_rank_v29&amp;spm=1018.2226.3001.4187 pyspider 框架底层使用什么原理进行匹配解析数据 答：pyspider 框架底层是利用 pyquery 库的语法进行数据匹配的 利用 pyspider 框架爬取数据的步骤 第一步：终端输入下面指令开启 pyspider 框架，如果出现 webui running on 0.0.0.0:5000 代表框架运行成功 指令： 1pyspider all 第二步：浏览器输入 localhost:5000 即可查看 pyspider 框架的 WebUl 第三步：点击右下角的 Create 即可创建爬虫项目 创建爬虫项目 第四步：创建完爬虫项目后进入下面界面开始写爬虫程序 第五步：运行写好的爬虫程序 pyspider 界面代码处的代码都有什么作用！ pyqpider 怎么将爬取的数据存储到 Mysql 数据库内 pyspider 框架怎么自动爬取全部数据并且保存为 json 格式或者 csv 文件 将数据保存为 json 格式或者 csv 文件","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享--Scrapy【第七期】","slug":"爬虫主流框架分享-Scrapy【第七期】","date":"2021-10-06T08:16:05.000Z","updated":"2021-10-06T09:34:21.667Z","comments":true,"path":"2021/10/06/爬虫主流框架分享-Scrapy【第七期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB-Scrapy%E3%80%90%E7%AC%AC%E4%B8%83%E6%9C%9F%E3%80%91/","excerpt":"文章主讲内容：基于Scrapy框架的分布式爬虫实现原理【最后一期】","text":"文章主讲内容：基于Scrapy框架的分布式爬虫实现原理【最后一期】 分布式爬虫的原理及基于scrapy怎么实现分布式爬虫 分布式爬虫的原理 分布式爬虫目的：多台服务器抓取一个爬虫项目，数据不重复，提高数据抓取速度和效率！ 原理：多台服务器共享一个 url 队列（redis集合），然后一起进行数据的抓取 原理实现图： 注意：多台服务器共享爬取队列是通过 scrapy_redis 模块创建的 redis 集合 你问我答：三台服务器都部署相同的爬虫项目，也就拥有相同数量的的爬虫url地址（假设500），当三台服务器将所有的url都经过自身调度器交给redis创建的集合（全局url队列）之后，集合里面有多少个url地址呢？ 答：有500个url地址，因为redis的集合有去重功能，三个相同的爬虫项目，那每台服务器上面的url都是一样的啊 内部流程图： 注意： 1：当url通过自身调度器进入到redis创建的集合中之后，redis会为这个url生成一个指纹（实现增量式爬虫以及url的去重机制） 2：自身调度器将url从redis集合中取出来之后，全局url队列会记录哪台服务器取走了哪个url地址，这样多台服务器协同工作时就不会出现取走相同的url并且下载的情况，也就实现了分布式！ 3：假如公司有四台服务器，做分布式爬虫时，一般需要一台服务器进行url地址的管理！！ 4：负责管理url地址的那台服务器也可以进行数据的抓取！！！ 实现分布式爬虫： 实现原理：因为 scrapy 自身的调度器不支持分布式爬虫，因此我们需要重写 scrapy 框架的调度器从而实现分布式爬虫 具体实现： 1：安装 scrapy_redis模块（用作分布式爬虫的 url 全局共享队列）→ 也就是重写scrapy的调度器 安装指令： 1sudo pip3 install scrapy_redis 须知：为什么要用redis的集合充当分布式爬虫的全局url共享队列呢？ 答：因为redis是基于内存存储的，url的进与出较快，另一个原因就是 redis的集合有去重功能 2：创建非分布式的 scrapy 爬虫 3：在 settings.py 文件内设置一些参数即可 4：多台服务器同时运行一个爬虫项目实现分布式 创建分布式爬虫的具体步骤 第一步：按照非分布式爬虫的流程创建scrapy爬虫 第二步：在settings.py文件内进行参数设置，将非分布式爬虫设置为分布式爬虫（具体参数设置看下面） 第三步：在多台服务器上面同时运行相同的爬虫文件 分布式爬虫在settings.py文件内怎么设置参数 参数设置： 重新指定调度器（重写调度器）： 1SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot; 重新指定去重机制： 1DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; 是否清除url指纹（默认False是清除）： 1SCHEDULER_PERSIST = True 解释：爬虫项目结束之后，是否将url指纹清空，不清空将url指纹存在redis数据库内这个参数很重要常常用作爬虫的暂停、回复、断点续爬，最重要是可以实现增量式爬虫！！！ 设置 redis 管道：ITEM_PIPELINES = { ‘scrapy_redis.pipelines.RedisPipeline’: 优先级系数,}解释：如果需要将爬取的数据存入到redis数据库内就直接设置这个项目管道即可存入，不存redis就不用设置 指定当前服务器连接到哪台服务器上面的redis数据库（连接充当url地址管理的那台服务器上面的redis）： REDIS_HOST = “IP地址” 【充当 url 地址管理的那台服务器的IP地址】REDIS_PORT = 端口号 【redis 端口号是 6379】 如果 redis 设置了密码就这样设置： REDIS_URL = “redis：//user：密码@用户名：PORT” RedisDesktopManager的使用 RedisDesktopManager 怎么连接 redis 数据库 RedisDesktopManager 怎么刷新 redis 数据库里面的数据 RedisDesktopManager 怎么查看数据库里面的数据等操作 分布式爬虫的实现【实操】 实现分布式爬虫前的准备 1：准备多台服务器【每台服务器上都安装了 redis数据库以及 scrapy 框架】 2：确保多台服务器都可以远程连接 url 地址管理的那台服务器的 redis 数据库 3：在任意一台服务器上可安装 RedisDesktopManager 图形化远程查看 redis 数据库内数据的变化 怎么远程连接 redis 数据库 安装前注意事项：待连接数据库主机以及连接数据库主机上面都需要安装 redis 数据库 远程连接指令： 1redis-cli -h 待连接 redis 数据库的 IP 地址 图示： 怎么解决服务器不能远程连接另外一台服务器的 redis 数据库 第一步：在待远程链接的服务器（ubuntu）上面打开 /etc/redis/redis.conf 这个文件【在主目录下打开】 第二步：将 bind 127.0.0.1这段代码注释 第三步：将保护模式关闭 第四步：重启 redis 服务 第五步：重新远程连接 redis 数据库 创建分布式爬虫的步骤 第一步：利用 scrapy 创建非分布式爬虫 第二步：更改 settings.py 文件内的数据将非分布式爬虫部署为分布式爬虫 第三步：将 scrapy 写好的爬虫项目复制到其他几台服务器上面（其他几台服务器必须都安装 scrapy 框架以及 redis 数据库和 scrapy_redis） 注意：多台服务器上面还要保证 python 的版本都要一致 第四步：其他几台服务器一起【同时】运行这个爬虫项目即可实现分布式 Ubuntu系统运行爬虫项目： 其他服务器【win】运行分布式爬虫文件 第一步：进入爬虫项目文件夹 第二步：进入爬虫项目文件夹后进入终端 第三步：输入指令运行爬虫文件 注意：多台服务器实现分布式爬虫每台服务器都需要安装 redis 数据库 当分布式爬虫运行 充当 url 地址管理的那台服务器上面的 redis 数据库内都存放了什么数据！ 1：数据存放在了那里？ 答：redis 数据库默认将数据存放至代号为 db0 那个数据库里面 2：db0 数据库里面都存放了哪些数据？ 答：存放了 url 地址指纹、以及爬取的数据 如何实现分布式增量式爬虫 实现原理：在分布式爬虫程序结束之后，不清除 url 指纹，下载再次运行这个分布式爬虫即可实现增量式爬虫 不清除指纹设置：在 settings.py 文件内设置 SCHEDULER_PERSIST 的参数值为 True 即不清除 url 指纹 运行分布式爬虫出现的问题解决 出现的问题：在多台服务器同时运行同一个爬虫项目，因为 python 版本的不同会出现 builtins.ValueError 这种错误！ 问题解决：将所有爬虫服务器上面的 python 版本都设为一致【具体操作请看 python 怎么卸载和升级】 redis_key部署分布式爬虫 redis_key 部署分布式爬虫与上面正常部署分布式爬虫相比有哪些不同? 不同点：正常分布式爬虫多台主机同时运行后就开始数据的抓取了，redis_key 部署的分布式爬虫多台主机同时运行后不开始数据的抓取而是同时监听 6023 端口 等待在 redis 数据库命令行内压入初始 url 地址多台主机才同时进行数据的抓取，这样更好的控制多台主机的同步性 redis_key 部署分布式爬虫的具体步骤 第一步：正常创建一个分布式爬虫【非分布式爬虫 → 设置 settings.py文件变成分布式爬虫】 第二步：在真的爬虫文件内导入 RdeisSpider 类，并且真的爬虫类继承这个类后，注释掉 start_urls 这段代码并设置 redis_key 的参数值 导入模块语法： 1from scrapy_redis.spiders import RedisSpider 第三步：多台服务器同时运行部署的分布式爬虫 windows服务器运行redis_key创建好的爬虫 ubuntu服务器运行redis_key创建好的爬虫 第四步：进入充当 url 地址管理的那台服务器的 redis 数据库内【命令行】并压入初始 url 地址实现多台主机同时抓取数据 压入初始 url 指令：LPUSH 设置的redis_key值 初始的url地址 注意：初始 url 地址就是注释的 start_urls 的 url 值 redis_key 部署分布式爬虫的缺点以及怎么解决这个缺点 缺点：redis_key 部署的分布式爬虫在数据抓取结束后程序不会终止 解决办法：在 settings.py 文件内设置 CLOSESPIDER_TIMEOUT = 3600 【到了3600时间后程序会自动退出】 持续更新中……","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享--Scrapy【第六期】","slug":"爬虫主流框架分享-Scrapy【第六期】","date":"2021-10-06T07:58:02.000Z","updated":"2021-10-06T08:15:16.744Z","comments":true,"path":"2021/10/06/爬虫主流框架分享-Scrapy【第六期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB-Scrapy%E3%80%90%E7%AC%AC%E5%85%AD%E6%9C%9F%E3%80%91/","excerpt":"文章主讲内容：Scrapy处理Post请求及其设置图片管道与文件管道！","text":"文章主讲内容：Scrapy处理Post请求及其设置图片管道与文件管道！ Scrapy 框架怎么发送 Get 以及 Post 请求 向网站服务器发送 Get 请求： 1yield scrapy.Request（url=….，meta=….，callback=…，dont_filter = True） 流程详解：将目标 url 交给调度器入队列，并且出队列经过引擎交给下载器，下载器向网站服务器发送 Get 请求，获取响应的对象交给解析函数处理 dont_filter参数功能： 值为False：当一个 url 经过调度器入队列并出队列会为这个 url 生成一个指纹，相同的url再次经过调度器时，将不在交由下载器下载，并返回响应对象！ 值为True：当一个 url 经过调度器入队列并出队列会为这个 url 生成一个指纹，相同的url再次经过调度器时，将继续交由下载器下载，并返回响应对象！ 向网站服务器发送 Post 请求： 1yield scrapy.FormRequest（url=…，formdata=…，meta=…，callback=…. , dont_filter = True） 流程详解：将目标url交给调度器入队列，并出队列，经过引擎交给下载器，下载器带着表单数据（formdata）向服务器网站发送Post请求，获取响应对象交给解析函数处理 Scrapy 框架发送 Post请求的注意事项【很重要】 注意事项：在真的爬虫文件内一定要重写 start_requests 方法，因为 start_requests 源码默认是以 Get 的方式发送请求 scrapy 框架向网站服务器发送Post请求的步骤！ 第一步：创建爬虫项目 scrapy startproject 爬虫项目名称 第二步：进入到爬虫文件项目文件夹内 cd 真爬虫文件项目 第三步：创建真的爬虫文件 scrapy genspider 真的爬虫文件名 待爬取网站域名 第四步：在 item.py 文件内定义待爬取数据的字段 第五步：在真的爬虫文件内重写 start_requests 方法向服务器网站发送 Post 请求，并由解析函数解析提取数据 第六步：在项目管道内对提取的数据进行处理 第七步：设置 settings.py 文件 第八步：启动爬虫 注意： 1：想要用Scrapy框架发送post请求一定要在真的爬虫文件内重写 strat_requests方法 2：表单数据的value值都是字符串，没有整形！！！！ Scrapy图片管道 Scrapy 保存图片的方法（图片管道）步骤 第一步：创建爬虫项目 第二步：进入到爬虫项目 第三步：创建真的爬虫文件 第四步：进入 items.py 文件内定义图片的链接以及名称字段（保存图片一般是这两个字段） 第五步：进入真的爬虫文件内，重写 srart_requests 方法实现多线程爬虫，获取到图片的链接以及名称之后通过 yield 交给项目管道 第六步：进入 pipelines.py 文件内，继承 Scrapy 的图片管道类，重写两个方法（如下图），实现批量图片的保存 第七步：进入 settings.py 文件内，设置图片保存的路径变量 IMAGES_STORE （文件夹路径）以及其他参数 第八步：运行爬虫文件！ Scrapy 图片管道（保存图片用）的几点注意事项 1：在pipelines.py文件内 管道类需要继承Scrapy为我们写好的图片管道类（ImagesPipelin）！ 2：我们需要重写继承的类的两个方法可实现图片的保存，即get_media_requests方法（用于图片下载）以及file_path方法（用于设置图片保存的名称） Scrapy文件管道 os.path.splitext（url）方法的功能 功能：将文件的下载链接拆分为两部分，用来获取文件的后缀 语法： 12import os文件后缀 = os.path.splitext（文件下载链接） 注意：文件下载链接为 https：//123456kkk.aaa.jpg 也可以获取到文件的后缀为 .jpg 利用 Scrapy 框架的文件管道保存网站中的文件流程 第一步：创建爬虫项目 第二步：进入到爬虫项目 第三步：创建真的爬虫文件 第四步：进入 items.py 文件内定义文件的链接以及名称字段（保存文件一般是这两个字段） 第五步：进入真的爬虫文件内，看情况重写 srart_requests 方法（多线程），获取到文件的链接以及名称之后通过 yield 交给项目管道 第六步：进入 pipelines.py 文件内，继承 Scrapy 的文件管道类，重写两个方法（如下图），实现批量文件的保存 第七步：进入 settings.py 文件内，设置 FILES_STORE字段（设置文件保存的路径）以及其它参数 第八步：运行爬虫文件！ Scrapy 框架的文件管道与图片管道在保存数据上有什么不同！ 1：继承的类不一样，图片是ImagesPipline，文件是 FilesPipline 2：文件在项目管道内重写的 file_path 方法与图片的方法略有不同（获取文件后缀名！） 持续更新中……","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享--Scrapy【第五期】","slug":"爬虫主流框架分享-Scrapy【第五期】","date":"2021-10-06T07:49:04.000Z","updated":"2021-10-06T07:56:54.195Z","comments":true,"path":"2021/10/06/爬虫主流框架分享-Scrapy【第五期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB-Scrapy%E3%80%90%E7%AC%AC%E4%BA%94%E6%9C%9F%E3%80%91/","excerpt":"文章主讲内容：scrapy设置下载器中间件","text":"文章主讲内容：scrapy设置下载器中间件 middlewares.py 的源代码解析 注意：我们通过下载器中间件伪装请求的时候，不在下载器中间件的源码（类）内做设置，按照源码的格式在重写新的类在伪装即可！！ middlewares.py 文件内怎么实现伪装User-Agent middlewares.py 文件内怎么实现用代理IP发送请求并且处理代理IP异常 middlewares.py 文件内怎么实现设置cookies 设置为中间件之后，怎么开启中间件！ settings.py 文件内有关于中间件参数的设置！","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享--Scrapy【第四期】","slug":"爬虫主流框架分享-Scrapy【第四期】","date":"2021-10-06T07:27:41.000Z","updated":"2021-10-06T07:48:46.351Z","comments":true,"path":"2021/10/06/爬虫主流框架分享-Scrapy【第四期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB-Scrapy%E3%80%90%E7%AC%AC%E5%9B%9B%E6%9C%9F%E3%80%91/","excerpt":"这已经是Scrapy的第四期教程啦，本期教程分享如何使用Scrapy实现多级页面的抓取！ Scrapy 爬取多级页面（二级页面）的代码结构思路","text":"这已经是Scrapy的第四期教程啦，本期教程分享如何使用Scrapy实现多级页面的抓取！ Scrapy 爬取多级页面（二级页面）的代码结构思路 爬取要求： 1：在瓜子二手车官网的一级页面内爬取车辆的名称、价格、链接 2：在每辆车的界面（二级页面）爬取车辆的里程、排量、变速箱 代码思路： 前言：在爬取一级页面的基础上升级代码 第一步：在 item.py 文件内定义二级页面需要抓取的数据字段！ 第二步【重点】：在真的爬虫文件内写代码实现多级页面的数据抓取（如下） 第三步：将爬取的数据（一级页面和二级页面）交给 pipelines.py 文件对数据进行保存处理 第四步：编辑 settings.py 文件内信息 第五步：运行爬虫 Scrapy 的爬虫文件内的不同解析函数如何传递数据实现共同处理数据的目的 实现指令： 1：dict1= {“key”：copy.deepcopy（待传递的参数）} 2：yield scrapy.Request（url=…，meta=dict1，callback=func1） 把dict1参数交给func1函数进行处理 注意：1：必须要用copy.deepcopy（）对待传递的参数进行拷贝，否则会造成一级页面抓取的数据重复 2：yield scrapy.Request（url=二级地址，meta=dict1，callback=func1）代码详解：二级页面url地址通过调度器入队列，由下载器下载，把响应对象交给 func1 函数进行处理，meta参数作为响应对象的属性传给func1函数，在func1函数内获取meta参数，可以这么写：响应对象.meta 即可 Scrapy 爬取多级页面（三级页面或多级页面）的代码结构思路","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享--Scrapy【第三期】","slug":"爬虫主流框架分享-Scrapy【第三期】","date":"2021-10-06T07:19:34.000Z","updated":"2021-10-06T07:27:01.588Z","comments":true,"path":"2021/10/06/爬虫主流框架分享-Scrapy【第三期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB-Scrapy%E3%80%90%E7%AC%AC%E4%B8%89%E6%9C%9F%E3%80%91/","excerpt":"这是Scrapy教程的第三期，本期教程实现抓取数据的持久化存储！ 存储须知","text":"这是Scrapy教程的第三期，本期教程实现抓取数据的持久化存储！ 存储须知 1：将数据持久化存储都在项目管道组件中实现，存储为json以及csv文件不需要（scrapy内置命令行存储） 2：scrapy特点是遇到错误程序不停止，继续执行 将爬虫文件提取的数据持久化存储为 .json 以及 .csv 文件 终端指令： 1：scrapy crawl 爬虫文件名 -o 文件名.csv 2：scrapy crawl 爬虫文件名 -o 文件名.json 注意：存储为 csv 以及 json 文件之后，每一列数据的字段就是在 items.py 中定义的字典的键值 将爬虫文件提取的数据持久化存储到 Mysql 数据库以及项目管道中 return item 的作用！！ 数据持久化存储到 Mysql 第一步：在项目管道组件文件（pipelines.py）内新建一个管道类用于将数据存储到 mysql 数据库内 第二步：在settings.py内定义连接Mysql数据库用的主机名、用户名的变量参数等等 第三步：在项目管道文件内导入.settings模块，并在类中创建 open_spider( )方法，注意方法名不能变，用于连接数据库 第四步：在类中创建 close_spider( ) 方法，注意方法名是固定的，用于断开数据库 第五步：在 process_item（）方法中将数据存储到 mysql 数据库内（不要忘记commit） 第六步：在 settings.py中设置管道以及其权重 第七步：运行爬虫文件，将数据存入！ 项目管道中process_item函数中 return item 的作用 当前管道（类）将数据处理结束后，将数据返回给其他管道接着对数据进行处理，例如先对数据打印，再将数据存到 mysql数据库内，具体谁先谁后看管道权重！！！ settings.py 文件内设置的参数信息功能","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享——Scrapy【第二期】","slug":"爬虫主流框架分享——Scrapy【第二期】","date":"2021-10-06T07:12:34.000Z","updated":"2021-10-06T07:19:04.665Z","comments":true,"path":"2021/10/06/爬虫主流框架分享——Scrapy【第二期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB%E2%80%94%E2%80%94Scrapy%E3%80%90%E7%AC%AC%E4%BA%8C%E6%9C%9F%E3%80%91/","excerpt":"咱么继续接着上一期的支持讲解Scrapy框架，这一期讲解Scrapy进行多线程抓取 Scrapy框架写爬虫项目（单线程）的一般具体流程 第一步：在指定文件夹下创建爬虫项目文件夹 指令： scrapy startproject 爬虫项目文件名（第一个字母大写）","text":"咱么继续接着上一期的支持讲解Scrapy框架，这一期讲解Scrapy进行多线程抓取 Scrapy框架写爬虫项目（单线程）的一般具体流程 第一步：在指定文件夹下创建爬虫项目文件夹 指令： scrapy startproject 爬虫项目文件名（第一个字母大写） 第二步：进入到爬虫文件夹创建真的爬虫文件 → scrapy genspider 真的爬虫文件名 待爬取网站域名 第三步：进入到 items.py 文件设置保存的数据字段名（字典的key值） 第四步：进入真的爬虫文件实现数据解析提取函数功能以及将处理好的数据传给管道文件及实现网站的继续跟进提取 注意：将地址交给调度器入队列【下载器下载】是 yield scrapy.Request（）方法！！！ 第五步：配置 pipelins.py（项目管道） 文件（对提取的数据作何处理） 注意：一定要在每个项目管道后面加上 return 这样才能保证每个项目管道都可被执行 第六步：配置 settings.py 文件 第七步：运行写好的爬虫文件 爬虫项目启动引擎从爬虫文件获取第一批 url 地址交给调度器入队列的底层原理（分析源代码）！！ Scrapy框架写爬虫项目（多任务）的常用具体流程（效率高） 注意：和单任务的步骤都一样，除了真的爬虫文件里面的代码不一样！！！ 真的爬虫文件代码演示：","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫主流框架分享——Scrapy【第一期】","slug":"爬虫主流框架分享——Scrapy【第一期】","date":"2021-10-06T06:45:55.000Z","updated":"2021-10-06T07:57:18.690Z","comments":true,"path":"2021/10/06/爬虫主流框架分享——Scrapy【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6%E5%88%86%E4%BA%AB%E2%80%94%E2%80%94Scrapy%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"这一期文章记录自己从接触 Scrapy 框架到了解框架基本五大组件再到最后利用 Scrapy + scrapy_redis 部署分布式的过程，同样也是一起 Scrapy 的教程，加深自己对 Scrapy 框架的理解！开始！","text":"这一期文章记录自己从接触 Scrapy 框架到了解框架基本五大组件再到最后利用 Scrapy + scrapy_redis 部署分布式的过程，同样也是一起 Scrapy 的教程，加深自己对 Scrapy 框架的理解！开始！ 什么是Scrapy 答：scrapy框架是爬虫框架，它使用Twisted异步网络库来处理网络请求（多任务），使用scrapy可以高效（爬取效率和开发效率）的完成数据的爬取 怎么安装Scrapy Windows：pip install Scrapyubuntu：pip3 install Scrapy scrapy 框架的五大组件功能及 scrapy 框架的工作流程 五大组件加上两大中间件 引擎组件（Scrapy Engine）：Scrapy框架的核心，负责其他组件之间的通信等 爬虫组件（Spiders）：负责目标数据的解析提取并将数据交给项目管道，如果提取的数据含有继续跟进的二级链接，就把二级链接交给引擎循环！ 调度器组件（Scheduler）：负责将待请求的url入队列，在出队列（交给引擎），出队列之前为这个url建立指纹，防止重复抓取 下载器组件（Downloader）：负责向目标url发送请求，并且向引擎返回 responses（响应对象） 项目管道组件（Item Pipeline）：负责处理在爬虫组件获得的数据！ 下载器中间件（Downloader Middlewares）：处在调度器组件与下载器组件中间，调度器将目标url出队列，还未送到下载器获取响应对象之前，被下载器中间件拦下，在下载器中间件你可以对请求进行包装（User-Agent等），然后再将包装好的传到下载器组件 爬虫中间件（Spiders Middlewares）：处在引擎组件以及爬虫组件之间，可以修改响应状态码等，一般不做修改 工作流程 文字描述：项目开始后，引擎先向爬虫文件索要第一批待爬取的请求地址，交给调度器入队列，调度器在将地址出队列交给引擎（已经建立好指纹），引擎将地址在交给下载器，下载器向网站服务器发送请求，获取响应对象再次给引擎，引擎将响应对象交给爬虫文件，爬虫文件开始进行数据的解析提取并将数据交给引擎，如果解析的数据中包含继续跟进的url地址，引擎就把地址再次交给调度器入队列，如果不包含就将数据交给项目管道对数据进行处理（保存等） 图示： scrapy 爬虫文件结构 创建爬虫项目指令：scrapy startproject 爬虫项目文件名 文件结构图示： 注意：spiders是一个文件夹，不是py文件，里面存放爬虫组件相关的文件 利用scrapy创建一个蜘蛛爬虫流程 第一步（创建一个爬虫项目）：scrapy startproject 爬虫文件名 第二步（进入到这个爬虫文件内）：cd 爬虫文件名 第三步（在爬虫项目文件内创建一个真的蜘蛛爬虫）：scrapy genspider 蜘蛛爬虫名 蜘蛛爬虫待爬取网站域名 注意： 1：安装好scrapy后，在哪个文件夹下面都可以创建爬虫项目 2：通过第三步创建真的蜘蛛爬虫文件，就是Scrapy五大组件中的爬虫组件 3：爬虫待抓取的网站域名，是以 www. 开头的，也就是去掉协议以及资源路径的地址！ 示例： Scrapy文件详解 真的爬虫文件（爬虫组件）中代码的含义 settings.py全局爬虫配置文件内参数详解 底层代码理解scrapy爬虫框架的运行流程 真的爬虫文件怎么解析提取目标数据 注意事项： 1：response对象调用xapth方法获得的是列表内嵌套的选择器对象，不是目标字符串，想要获取字符串，就要在后面加上get（）方法 2：在后面加上extract（）方法就是将所有选择器对象中的字符串存到列表内！ 3：选择器对象的get方法是获取列表内第一个选择器的文本内容！！！ 怎么在终端以及Pycharm中运行scrapy写好的爬虫 终端运行指令：回到爬虫目录文件夹处，键入指令 scrapy crawl 真的爬虫文件名（不带py后缀） pycharm运行：在爬虫文件夹根目录新建一个 run.py 文件，文件内代码如下即可运行写好的爬虫！","categories":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"}]},{"title":"爬虫抓包工具分享【第一期】","slug":"爬虫抓包工具分享【第一期】","date":"2021-10-06T02:49:18.000Z","updated":"2021-10-06T06:43:47.772Z","comments":true,"path":"2021/10/06/爬虫抓包工具分享【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%88%86%E4%BA%AB%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"在我们精进爬虫的过程中，肯定需要用到额外的抓包工具，今天就给大家带来几款电脑端常用的抓包工具！ Fiddler","text":"在我们精进爬虫的过程中，肯定需要用到额外的抓包工具，今天就给大家带来几款电脑端常用的抓包工具！ Fiddler 支持平台：Windows抓包平台：Windows、手机端软件官网：https://www.telerik.com/download/fiddler软件插件下载网站：https://www.telerik.com/fiddler/add-ons缺点：麦金塔电脑不支持使用！ Charles 支持平台：Windows、麦金塔、Linux抓包平台：Windows、苹果端、手机端软件官网【未激活】：https://www.charlesproxy.com/latest-release/download.do激活软件网站：https://www.zzzmode.com/mytools/charles/优点：支持反向代理、解析AFM协议等 Mitmproxy 抓包平台：Windows、Linux官网：https://mitmproxy.org/安装指令【win】：pip3 install mitmproxy构成：mitmproxy【win不支持】、mitmdump、mitmweb注意：这个抓包工具以指令行形式工作，可以与Python无缝交互！ HttpCanary 手机端抓包工具…… 持续更新中…..","categories":[{"name":"爬虫抓包常用工具","slug":"爬虫抓包常用工具","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8C%85%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"爬虫抓包常用工具","slug":"爬虫抓包常用工具","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8C%85%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"}]},{"title":"Github热门的Python项目分享【第一期】","slug":"Github热门的Python项目分享【第一期】","date":"2021-10-06T02:38:26.000Z","updated":"2021-10-06T06:42:52.267Z","comments":true,"path":"2021/10/06/Github热门的Python项目分享【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/Github%E7%83%AD%E9%97%A8%E7%9A%84Python%E9%A1%B9%E7%9B%AE%E5%88%86%E4%BA%AB%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"今天给大家带来了几个在 Github 上非常有热度的 Python 项目~ 100天成为Python高手 详细讲解Python的语法及其各种应用场景 https://github.com/jackfrued/Python-100-Days","text":"今天给大家带来了几个在 Github 上非常有热度的 Python 项目~ 100天成为Python高手 详细讲解Python的语法及其各种应用场景 https://github.com/jackfrued/Python-100-Days Python热门库 Python热门库大全分享 https://github.com/vinta/awesome-python Python各种API分享 API接口直接调用！ https://github.com/public-apis/public-apis Python基础课程 50节Python基础课程 https://github.com/jackfrued/Python-Core-50-Courses Python有趣的项目分享 各种Python项目 https://github.com/521xueweihan/HelloGitHub Python资源大全【力荐】 Web 框架、网络爬虫、网络内容提取、模板引擎、数据库应有尽有 https://github.com/jobbole/awesome-python-cn 持续更新中……","categories":[{"name":"Github","slug":"Github","permalink":"http://pythonlamb.github.io/categories/Github/"}],"tags":[{"name":"Github","slug":"Github","permalink":"http://pythonlamb.github.io/tags/Github/"}]},{"title":"Github查看热门项目排名第一期","slug":"Github","date":"2021-10-06T02:27:56.000Z","updated":"2021-10-06T02:36:43.409Z","comments":true,"path":"2021/10/06/Github/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/Github/","excerpt":"在程序猿的工作中肯定少不了与全球最大的同性交友网站Github打交道，这个网站里面有很多优质的项目源码及其教程，今天就分享给大家如何查看 Github 的热门排名项目【通过网站查看】叭~ 爱资料工具 通过爬虫技术获取数据并且整理排名 https://www.toolnb.com/tools/githubStarRanking.html","text":"在程序猿的工作中肯定少不了与全球最大的同性交友网站Github打交道，这个网站里面有很多优质的项目源码及其教程，今天就分享给大家如何查看 Github 的热门排名项目【通过网站查看】叭~ 爱资料工具 通过爬虫技术获取数据并且整理排名 https://www.toolnb.com/tools/githubStarRanking.html Github内部排名 *这里只分享 python 项目的排名网站 https://github.com/trending/python Github内部项目的排名 *在Github内的可以查看项目排名仓库 1：https://github.com/kon9chunkit/GitHub-Chinese-Top-Charts 2：https://github.com/osmboy/github-rank 持续更新中……","categories":[{"name":"Github","slug":"Github","permalink":"http://pythonlamb.github.io/categories/Github/"}],"tags":[{"name":"Github","slug":"Github","permalink":"http://pythonlamb.github.io/tags/Github/"}]},{"title":"在线学习网站分享【第一期】","slug":"在线学习爬虫网站分享【第一期】","date":"2021-10-06T02:07:01.000Z","updated":"2021-10-06T02:24:25.157Z","comments":true,"path":"2021/10/06/在线学习爬虫网站分享【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/06/%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0%E7%88%AC%E8%99%AB%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"记录一下自己经常使用的编程学习网站~ 网站类 哔哩哔哩【首推】 自认为全网最全的学习网站，各种资源应有尽有 https://www.bilibili.com/","text":"记录一下自己经常使用的编程学习网站~ 网站类 哔哩哔哩【首推】 自认为全网最全的学习网站，各种资源应有尽有 https://www.bilibili.com/ 菜鸟教程 精简的各种教程，支持代码在线运行等 https://www.runoob.com/ W3chool 前端教程的扛把子 https://www.w3school.com.cn/ 社区类 CSND 找一些问题的解决办法还是可以的，但良莠不齐 https://www.csdn.net/ 博客园 里面的教程文章也还可以 https://www.cnblogs.com/ 掘金 用的不多单 https://juejin.cn/frontend V2EX 俗称程序员摸鱼社区 https://www.v2ex.com/ Stack Overflow 全球最大的解决问题社区【需要翻墙因此放后面了】 https://stackoverflow.com/ 程序员客栈 用的不多 https://www.proginn.com/ 刷题与面经 牛客网 *中国很出名刷题平台/ https://www.nowcoder.com/ 持续更新中……","categories":[{"name":"学习网站","slug":"学习网站","permalink":"http://pythonlamb.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"学习网站","slug":"学习网站","permalink":"http://pythonlamb.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"}]},{"title":"学习爬虫的论坛分享【第一期】","slug":"学习爬虫的论坛分享【第一期】","date":"2021-10-05T12:46:32.000Z","updated":"2021-10-06T02:02:20.669Z","comments":true,"path":"2021/10/05/学习爬虫的论坛分享【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/05/%E5%AD%A6%E4%B9%A0%E7%88%AC%E8%99%AB%E7%9A%84%E8%AE%BA%E5%9D%9B%E5%88%86%E4%BA%AB%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"打卡记录一下自己经常在学习爬虫中用到学习论坛及各位大佬的私人博客~ 爬虫安全论坛【首推】 https://bbs.nightteam.cn/","text":"打卡记录一下自己经常在学习爬虫中用到学习论坛及各位大佬的私人博客~ 爬虫安全论坛【首推】 https://bbs.nightteam.cn/ Github Github内搜索爬虫关键字即可查看相关文档！ https://github.com/ 静觅大佬的博客 虫领域出名的大佬 https://cuiqingcai.com/ 猿人学 超级超级崇拜的大佬博客【王老师】JS逆向水平一流！ https://www.yuanrenxue.com/ Gitee 中国版的代码托管平台，里面也有很多优秀教程 https://gitee.com/ 廖雪峰的官方网站 https://www.liaoxuefeng.com/wiki/1016959663602400 精易论坛 https://bbs.125.la/ 持续更新中…..","categories":[{"name":"爬虫论坛","slug":"爬虫论坛","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E8%AE%BA%E5%9D%9B/"}],"tags":[{"name":"爬虫论坛","slug":"爬虫论坛","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E8%AE%BA%E5%9D%9B/"}]},{"title":"爬虫使用的在线工具网站分享【第一期】","slug":"爬虫使用的在线工具网站分享【第一期】","date":"2021-10-04T11:47:26.000Z","updated":"2021-10-05T12:12:25.510Z","comments":true,"path":"2021/10/04/爬虫使用的在线工具网站分享【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/10/04/%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"大家好！我是一名非专业的爬虫攻城狮，因为热爱，所以坚持，今天将给大家带来几款超级实用的在线网站资源，这些网站将会在你编写爬虫程序时带来很大的遍历，分享给大家的通知自己也记录一下！ 网站分享 在线User-Agent生成 1https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;createuseragent.html","text":"大家好！我是一名非专业的爬虫攻城狮，因为热爱，所以坚持，今天将给大家带来几款超级实用的在线网站资源，这些网站将会在你编写爬虫程序时带来很大的遍历，分享给大家的通知自己也记录一下！ 网站分享 在线User-Agent生成 1https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;createuseragent.html 在线构造Headers 1https:&#x2F;&#x2F;curl.trillworks.com&#x2F; Xpath测试 1https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;xpath.html 模拟请求 1http:&#x2F;&#x2F;www.fly63.com&#x2F;php&#x2F;http&#x2F; url格式化 1https:&#x2F;&#x2F;tools.miku.ac&#x2F;url_format&#x2F; 正则大全 1https:&#x2F;&#x2F;tools.miku.ac&#x2F;any_rule&#x2F; 测试正则 1https:&#x2F;&#x2F;regex101.com&#x2F; Linux指令查询 1https:&#x2F;&#x2F;tools.miku.ac&#x2F;linux_command&#x2F; 获取网页源码 123https:&#x2F;&#x2F;tools.miku.ac&#x2F;get_html&#x2F;https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;getHttpData.html 解析User-Agent 1https:&#x2F;&#x2F;tools.miku.ac&#x2F;ua_parser&#x2F; 时间戳转换 1https:&#x2F;&#x2F;tools.miku.ac&#x2F;timestamp&#x2F; Query测试 1https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;querylist.html WebSocket测试 123http:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;webSocketTools.htmlhttps:&#x2F;&#x2F;www.toolfk.com&#x2F;tool-online-runwebsocket.html CSV格式转换 123https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;csvconvert.htmlhttps:&#x2F;&#x2F;www.toolfk.com&#x2F;tool-convert-csv.html Http状态码含义 1https:&#x2F;&#x2F;www.toolnb.com&#x2F;tools&#x2F;getHttpStatusCode.html 在线Scrapy项目 1https:&#x2F;&#x2F;www.toolnb.com&#x2F;dev&#x2F;Scrapy.html 编码解码 123http:&#x2F;&#x2F;tool.mkblog.cn&#x2F;unicode&#x2F;https:&#x2F;&#x2F;www.toolfk.com&#x2F;tool-encdec-transform.html 文本对比 12345https:&#x2F;&#x2F;tool.oschina.net&#x2F;diffhttps:&#x2F;&#x2F;text-compare.com&#x2F;zh-hans&#x2F;https:&#x2F;&#x2F;www.toolfk.com&#x2F;tool-online-difftext.html 持续更新中……..","categories":[{"name":"在线工具","slug":"在线工具","permalink":"http://pythonlamb.github.io/categories/%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"爬虫在线工具网站","slug":"爬虫在线工具网站","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/"}]},{"title":"爬虫练习平台分享【第一期】","slug":"爬虫练习平台分享【第一期】","date":"2021-09-30T12:40:26.000Z","updated":"2021-10-06T02:23:44.235Z","comments":true,"path":"2021/09/30/爬虫练习平台分享【第一期】/","link":"","permalink":"http://pythonlamb.github.io/2021/09/30/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0%E5%B9%B3%E5%8F%B0%E5%88%86%E4%BA%AB%E3%80%90%E7%AC%AC%E4%B8%80%E6%9C%9F%E3%80%91/","excerpt":"大家好！今天给大家带来了两款在线练习爬虫的平台，一款是综合爬虫在线练习平台，包含Cookies模拟登录、简单反爬、IP池的构建，动态页面的抓取等，另外一款则偏向于JS逆向方面的练习，通过这两个平台的综合练习我相信小伙伴的爬虫技术会越来越精进的哦，希望大家喜欢~ Scrape Center","text":"大家好！今天给大家带来了两款在线练习爬虫的平台，一款是综合爬虫在线练习平台，包含Cookies模拟登录、简单反爬、IP池的构建，动态页面的抓取等，另外一款则偏向于JS逆向方面的练习，通过这两个平台的综合练习我相信小伙伴的爬虫技术会越来越精进的哦，希望大家喜欢~ Scrape Center 综合爬虫练习平台： https://scrape.center/ 猿人学WEB刷题 JS逆向练习平台 https://match.yuanrenxue.com/list 持续更新中…….","categories":[{"name":"爬虫练习","slug":"爬虫练习","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0/"}],"tags":[{"name":"爬虫练习","slug":"爬虫练习","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0/"}]}],"categories":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"},{"name":"爬虫练习","slug":"爬虫练习","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0/"},{"name":"常用类库篇","slug":"常用类库篇","permalink":"http://pythonlamb.github.io/categories/%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%BA%93%E7%AF%87/"},{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"},{"name":"爬虫抓包常用工具","slug":"爬虫抓包常用工具","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8C%85%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"},{"name":"Github","slug":"Github","permalink":"http://pythonlamb.github.io/categories/Github/"},{"name":"学习网站","slug":"学习网站","permalink":"http://pythonlamb.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"},{"name":"爬虫论坛","slug":"爬虫论坛","permalink":"http://pythonlamb.github.io/categories/%E7%88%AC%E8%99%AB%E8%AE%BA%E5%9D%9B/"},{"name":"在线工具","slug":"在线工具","permalink":"http://pythonlamb.github.io/categories/%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"数据存储篇","slug":"数据存储篇","permalink":"http://pythonlamb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%AF%87/"},{"name":"爬虫练习","slug":"爬虫练习","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E7%BB%83%E4%B9%A0/"},{"name":"常用类库篇","slug":"常用类库篇","permalink":"http://pythonlamb.github.io/tags/%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%BA%93%E7%AF%87/"},{"name":"爬虫主流框架","slug":"爬虫主流框架","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E4%B8%BB%E6%B5%81%E6%A1%86%E6%9E%B6/"},{"name":"爬虫抓包常用工具","slug":"爬虫抓包常用工具","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8C%85%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"},{"name":"Github","slug":"Github","permalink":"http://pythonlamb.github.io/tags/Github/"},{"name":"学习网站","slug":"学习网站","permalink":"http://pythonlamb.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"},{"name":"爬虫论坛","slug":"爬虫论坛","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E8%AE%BA%E5%9D%9B/"},{"name":"爬虫在线工具网站","slug":"爬虫在线工具网站","permalink":"http://pythonlamb.github.io/tags/%E7%88%AC%E8%99%AB%E5%9C%A8%E7%BA%BF%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/"}]}